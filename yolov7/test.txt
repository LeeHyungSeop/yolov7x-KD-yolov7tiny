YOLOR 🚀 v0.1-128-ga207844 torch 2.4.0 CUDA:0 (NVIDIA GeForce RTX 3060, 12287.5MB)

Namespace(weights='yolov7x.pt', cfg='cfg/training/yolov7x.yaml', data='../dataset/data.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=300, batch_size=4, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7x-custom', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7x-custom23', total_batch_size=4)
[34m[1mtensorboard: [0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/
[34m[1mhyperparameters: [0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1
/home/hslee/YOLOv7_KD/yolov7/train.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  run_id = torch.load(weights, map_location=device).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None
/home/hslee/YOLOv7_KD/yolov7/train.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(weights, map_location=device)  # load checkpoint
Overriding model.yaml nc=80 with nc=1

                 from  n    params  module                                  arguments                     
  0                -1  1      1160  models.common.Conv                      [3, 40, 3, 1]                 
  1                -1  1     28960  models.common.Conv                      [40, 80, 3, 2]                
  2                -1  1     57760  models.common.Conv                      [80, 80, 3, 1]                
  3                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               
  4                -1  1     10368  models.common.Conv                      [160, 64, 1, 1]               
  5                -2  1     10368  models.common.Conv                      [160, 64, 1, 1]               
  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                
  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                
  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                
  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                
 10                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                
 11                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                
 12[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
 13                -1  1    103040  models.common.Conv                      [320, 320, 1, 1]              
 14                -1  1         0  models.common.MP                        []                            
 15                -1  1     51520  models.common.Conv                      [320, 160, 1, 1]              
 16                -3  1     51520  models.common.Conv                      [320, 160, 1, 1]              
 17                -1  1    230720  models.common.Conv                      [160, 160, 3, 2]              
 18          [-1, -3]  1         0  models.common.Concat                    [1]                           
 19                -1  1     41216  models.common.Conv                      [320, 128, 1, 1]              
 20                -2  1     41216  models.common.Conv                      [320, 128, 1, 1]              
 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 23                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 24                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 25                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 26                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 27[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
 28                -1  1    410880  models.common.Conv                      [640, 640, 1, 1]              
 29                -1  1         0  models.common.MP                        []                            
 30                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              
 31                -3  1    205440  models.common.Conv                      [640, 320, 1, 1]              
 32                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              
 33          [-1, -3]  1         0  models.common.Concat                    [1]                           
 34                -1  1    164352  models.common.Conv                      [640, 256, 1, 1]              
 35                -2  1    164352  models.common.Conv                      [640, 256, 1, 1]              
 36                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 37                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 38                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 39                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 40                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 41                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 42[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
 43                -1  1   1640960  models.common.Conv                      [1280, 1280, 1, 1]            
 44                -1  1         0  models.common.MP                        []                            
 45                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             
 46                -3  1    820480  models.common.Conv                      [1280, 640, 1, 1]             
 47                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              
 48          [-1, -3]  1         0  models.common.Concat                    [1]                           
 49                -1  1    328192  models.common.Conv                      [1280, 256, 1, 1]             
 50                -2  1    328192  models.common.Conv                      [1280, 256, 1, 1]             
 51                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 52                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 53                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 54                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 55                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 56                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 57[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
 58                -1  1   1640960  models.common.Conv                      [1280, 1280, 1, 1]            
 59                -1  1  11887360  models.common.SPPCSPC                   [1280, 640, 1]                
 60                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              
 61                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 62                43  1    410240  models.common.Conv                      [1280, 320, 1, 1]             
 63          [-1, -2]  1         0  models.common.Concat                    [1]                           
 64                -1  1    164352  models.common.Conv                      [640, 256, 1, 1]              
 65                -2  1    164352  models.common.Conv                      [640, 256, 1, 1]              
 66                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 67                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 68                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 69                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 70                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 71                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 72[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
 73                -1  1    410240  models.common.Conv                      [1280, 320, 1, 1]             
 74                -1  1     51520  models.common.Conv                      [320, 160, 1, 1]              
 75                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 76                28  1    102720  models.common.Conv                      [640, 160, 1, 1]              
 77          [-1, -2]  1         0  models.common.Concat                    [1]                           
 78                -1  1     41216  models.common.Conv                      [320, 128, 1, 1]              
 79                -2  1     41216  models.common.Conv                      [320, 128, 1, 1]              
 80                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 81                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 82                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 83                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              
 86[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
 87                -1  1    102720  models.common.Conv                      [640, 160, 1, 1]              
 88                -1  1         0  models.common.MP                        []                            
 89                -1  1     25920  models.common.Conv                      [160, 160, 1, 1]              
 90                -3  1     25920  models.common.Conv                      [160, 160, 1, 1]              
 91                -1  1    230720  models.common.Conv                      [160, 160, 3, 2]              
 92      [-1, -3, 73]  1         0  models.common.Concat                    [1]                           
 93                -1  1    164352  models.common.Conv                      [640, 256, 1, 1]              
 94                -2  1    164352  models.common.Conv                      [640, 256, 1, 1]              
 95                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 96                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
100                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              
101[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
102                -1  1    410240  models.common.Conv                      [1280, 320, 1, 1]             
103                -1  1         0  models.common.MP                        []                            
104                -1  1    103040  models.common.Conv                      [320, 320, 1, 1]              
105                -3  1    103040  models.common.Conv                      [320, 320, 1, 1]              
106                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              
107      [-1, -3, 59]  1         0  models.common.Concat                    [1]                           
108                -1  1    656384  models.common.Conv                      [1280, 512, 1, 1]             
109                -2  1    656384  models.common.Conv                      [1280, 512, 1, 1]             
110                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              
111                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              
112                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              
113                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              
114                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              
115                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              
116[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           
117                -1  1   1639680  models.common.Conv                      [2560, 640, 1, 1]             
118                87  1    461440  models.common.Conv                      [160, 320, 3, 1]              
119               102  1   1844480  models.common.Conv                      [320, 640, 3, 1]              
120               117  1   7375360  models.common.Conv                      [640, 1280, 3, 1]             
121   [118, 119, 120]  1     42668  models.yolo.IDetect                     [1, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [320, 640, 1280]]
/root/anaconda3/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538438429/work/aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Model Summary: 467 layers, 70815092 parameters, 70815092 gradients, 188.9 GFLOPS

Transferred 630/644 items from yolov7x.pt
Scaled weight_decay = 0.0005
Optimizer groups: 108 .bias, 108 conv.weight, 111 other
/home/hslee/YOLOv7_KD/yolov7/utils/datasets.py:394: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  cache, exists = torch.load(cache_path), True  # load
[34m[1mwandb: [0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)
[34m[1mtrain: [0mScanning '../dataset/train.cache' images and labels... 8000 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 8000/8000 [00:00<?, ?it/s][34m[1mtrain: [0mScanning '../dataset/train.cache' images and labels... 8000 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 8000/8000 [00:00<?, ?it/s]
[34m[1mval: [0mScanning '../dataset/val.cache' images and labels... 2000 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 2000/2000 [00:00<?, ?it/s][34m[1mval: [0mScanning '../dataset/val.cache' images and labels... 2000 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 2000/2000 [00:00<?, ?it/s]
/home/hslee/YOLOv7_KD/yolov7/train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = amp.GradScaler(enabled=cuda)
Image sizes 640 train, 640 test
Using 4 dataloader workers
Logging results to runs/train/yolov7x-custom23
Starting training for 300 epochs...

     Epoch   gpu_mem       box       obj       cls     total    labels  img_size

[34m[1mautoanchor: [0mAnalyzing anchors... anchors/target = 4.42, Best Possible Recall (BPR) = 0.9962
  0%|          | 0/2000 [00:00<?, ?it/s]/home/hslee/YOLOv7_KD/yolov7/train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(enabled=cuda):
     0/299     1.56G   0.08831   0.02418         0    0.1125        34       640:   0%|          | 0/2000 [00:04<?, ?it/s]     0/299     1.56G   0.08831   0.02418         0    0.1125        34       640:   0%|          | 1/2000 [00:04<2:21:12,  4.24s/it]     0/299     5.17G   0.08824   0.02265         0    0.1109        25       640:   0%|          | 1/2000 [00:04<2:21:12,  4.24s/it]     0/299     5.17G   0.08824   0.02265         0    0.1109        25       640:   0%|          | 2/2000 [00:04<1:04:01,  1.92s/it]     0/299     5.17G   0.09048   0.02092         0    0.1114        17       640:   0%|          | 2/2000 [00:04<1:04:01,  1.92s/it]     0/299     5.17G   0.09048   0.02092         0    0.1114        17       640:   0%|          | 3/2000 [00:04<38:45,  1.16s/it]       0/299     5.17G   0.08888   0.02144         0    0.1103        28       640:   0%|          | 3/2000 [00:05<38:45,  1.16s/it]     0/299     5.17G   0.08888   0.02144         0    0.1103        28       640:   0%|          | 4/2000 [00:05<26:52,  1.24it/s]     0/299     5.17G   0.08579   0.02083         0    0.1066         8       640:   0%|          | 4/2000 [00:05<26:52,  1.24it/s]     0/299     5.17G   0.08579   0.02083         0    0.1066         8       640:   0%|          | 5/2000 [00:05<20:18,  1.64it/s]     0/299     5.17G   0.08513   0.02058         0    0.1057        19       640:   0%|          | 5/2000 [00:05<20:18,  1.64it/s]     0/299     5.17G   0.08513   0.02058         0    0.1057        19       640:   0%|          | 6/2000 [00:05<16:23,  2.03it/s]     0/299     5.17G   0.08472    0.0207         0    0.1054        26       640:   0%|          | 6/2000 [00:05<16:23,  2.03it/s]     0/299     5.17G   0.08472    0.0207         0    0.1054        26       640:   0%|          | 7/2000 [00:05<13:58,  2.38it/s]     0/299     5.17G   0.08621   0.02049         0    0.1067        25       640:   0%|          | 7/2000 [00:06<13:58,  2.38it/s]     0/299     5.17G   0.08621   0.02049         0    0.1067        25       640:   0%|          | 8/2000 [00:06<12:18,  2.70it/s]     0/299     5.17G   0.08657   0.02085         0    0.1074        39       640:   0%|          | 8/2000 [00:06<12:18,  2.70it/s]     0/299     5.17G   0.08657   0.02085         0    0.1074        39       640:   0%|          | 9/2000 [00:06<11:10,  2.97it/s]     0/299     5.17G   0.08625   0.02142         0    0.1077        50       640:   0%|          | 9/2000 [00:06<11:10,  2.97it/s]     0/299     5.17G   0.08625   0.02142         0    0.1077        50       640:   0%|          | 10/2000 [00:06<10:29,  3.16it/s]     0/299     5.17G   0.08474   0.02107         0    0.1058         8       640:   0%|          | 10/2000 [00:06<10:29,  3.16it/s]     0/299     5.17G   0.08474   0.02107         0    0.1058         8       640:   1%|          | 11/2000 [00:06<09:55,  3.34it/s]     0/299     5.17G   0.08457   0.02147         0     0.106        51       640:   1%|          | 11/2000 [00:07<09:55,  3.34it/s]     0/299     5.17G   0.08457   0.02147         0     0.106        51       640:   1%|          | 12/2000 [00:07<09:30,  3.49it/s]     0/299     5.17G   0.08384   0.02169         0    0.1055        27       640:   1%|          | 12/2000 [00:07<09:30,  3.49it/s]     0/299     5.17G   0.08384   0.02169         0    0.1055        27       640:   1%|          | 13/2000 [00:07<09:15,  3.58it/s]     0/299     5.17G   0.08428   0.02156         0    0.1058        23       640:   1%|          | 13/2000 [00:07<09:15,  3.58it/s]     0/299     5.17G   0.08428   0.02156         0    0.1058        23       640:   1%|          | 14/2000 [00:07<09:02,  3.66it/s]     0/299     5.17G   0.08379    0.0218         0    0.1056        33       640:   1%|          | 14/2000 [00:07<09:02,  3.66it/s]     0/299     5.17G   0.08379    0.0218         0    0.1056        33       640:   1%|          | 15/2000 [00:07<08:55,  3.71it/s]     0/299     5.17G   0.08382   0.02187         0    0.1057        35       640:   1%|          | 15/2000 [00:08<08:55,  3.71it/s]     0/299     5.17G   0.08382   0.02187         0    0.1057        35       640:   1%|          | 16/2000 [00:08<08:50,  3.74it/s]     0/299     5.17G   0.08332     0.022         0    0.1053        30       640:   1%|          | 16/2000 [00:08<08:50,  3.74it/s]     0/299     5.17G   0.08332     0.022         0    0.1053        30       640:   1%|          | 17/2000 [00:08<08:43,  3.78it/s]     0/299     5.17G   0.08359   0.02204         0    0.1056        39       640:   1%|          | 17/2000 [00:08<08:43,  3.78it/s]     0/299     5.17G   0.08359   0.02204         0    0.1056        39       640:   1%|          | 18/2000 [00:08<08:39,  3.81it/s]     0/299     5.17G   0.08372   0.02195         0    0.1057        25       640:   1%|          | 18/2000 [00:08<08:39,  3.81it/s]     0/299     5.17G   0.08372   0.02195         0    0.1057        25       640:   1%|          | 19/2000 [00:08<08:37,  3.83it/s]     0/299     5.17G   0.08378   0.02179         0    0.1056        26       640:   1%|          | 19/2000 [00:09<08:37,  3.83it/s]     0/299     5.17G   0.08378   0.02179         0    0.1056        26       640:   1%|          | 20/2000 [00:09<08:35,  3.84it/s]     0/299     5.17G   0.08348   0.02165         0    0.1051        14       640:   1%|          | 20/2000 [00:09<08:35,  3.84it/s]     0/299     5.17G   0.08348   0.02165         0    0.1051        14       640:   1%|          | 21/2000 [00:09<08:32,  3.86it/s]     0/299     5.17G   0.08345    0.0218         0    0.1052        46       640:   1%|          | 21/2000 [00:09<08:32,  3.86it/s]     0/299     5.17G   0.08345    0.0218         0    0.1052        46       640:   1%|          | 22/2000 [00:09<08:40,  3.80it/s]     0/299     5.17G   0.08358   0.02171         0    0.1053        23       640:   1%|          | 22/2000 [00:10<08:40,  3.80it/s]     0/299     5.17G   0.08358   0.02171         0    0.1053        23       640:   1%|          | 23/2000 [00:10<08:59,  3.66it/s]     0/299     5.17G    0.0835   0.02162         0    0.1051        19       640:   1%|          | 23/2000 [00:10<08:59,  3.66it/s]     0/299     5.17G    0.0835   0.02162         0    0.1051        19       640:   1%|          | 24/2000 [00:10<09:15,  3.55it/s]     0/299     5.17G   0.08324   0.02166         0    0.1049        30       640:   1%|          | 24/2000 [00:10<09:15,  3.55it/s]     0/299     5.17G   0.08324   0.02166         0    0.1049        30       640:   1%|▏         | 25/2000 [00:10<09:55,  3.32it/s]     0/299     5.17G    0.0834   0.02154         0    0.1049        21       640:   1%|▏         | 25/2000 [00:11<09:55,  3.32it/s]     0/299     5.17G    0.0834   0.02154         0    0.1049        21       640:   1%|▏         | 26/2000 [00:11<10:03,  3.27it/s]     0/299     5.17G   0.08352    0.0215         0     0.105        37       640:   1%|▏         | 26/2000 [00:11<10:03,  3.27it/s]     0/299     5.17G   0.08352    0.0215         0     0.105        37       640:   1%|▏         | 27/2000 [00:11<09:40,  3.40it/s]     0/299     5.17G   0.08315   0.02156         0    0.1047        26       640:   1%|▏         | 27/2000 [00:11<09:40,  3.40it/s]     0/299     5.17G   0.08315   0.02156         0    0.1047        26       640:   1%|▏         | 28/2000 [00:11<09:16,  3.54it/s]     0/299     5.17G   0.08329   0.02157         0    0.1049        37       640:   1%|▏         | 28/2000 [00:11<09:16,  3.54it/s]     0/299     5.17G   0.08329   0.02157         0    0.1049        37       640:   1%|▏         | 29/2000 [00:11<09:01,  3.64it/s]